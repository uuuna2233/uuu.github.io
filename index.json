[{"categories":["Post"],"contents":"● 非關聯式資料庫 (NoSQL)\nMongoDB 是文件導向式資料庫，儲存由 key-value 配對而成的資料，無需事先定義資料型態，對於不同表徵的資料，都能快速存進資料庫\n● BSON 格式\n以 JSON 字串存進資料庫，MongoDB 內部會將 JSON 轉成 BSON，由 text 格式轉成 binary 格式，加快文件解析速度，亦能儲存非文字資料\n● _id 欄位\nMongoDB 會為每筆資料自動建立 _id 欄位，預設內容為 ObjectID，可視為主索引鍵，用於區分不同 Document，ObjectID 前 4 個 bytes 為 Document 產生時的時間戳記\nRDBMS MongoDB Remarks Database Database 資料庫 Table Collection 資料表/聚集 Row Document 資料/文件 Column Field 欄位/鍵名 Primary _id 主鍵/主索引 View View 視觀表 安裝 MongoDB on Windows MongoDB 可於本地安裝 Community Server，也能使用 Atlas 在 AWS、Azure 或 GCP 建立雲端託管服務\n本次選擇本地 (Windows) 下載 msi 安裝檔，預設執行檔會放在 C:\\Program Files\\MongoDB\\Server\\[version]\\bin，此次將資料庫存放到 D 槽\n將 Windows 服務裡的 MongoDB 項目改為手動（正式上線使用前，改成手動以免重開機後 MongoDB server 自動啟動)\n可以安裝 GUI 工具 compass 和 Shell 工具 mongosh，直接操作 MongoDB 打開命令提示字元並輸入 mongosh.exe 以連接 MongoDB，指令相當於 mongosh \u0026ldquo;mongodb://localhost:27017\u0026rdquo;\n安裝 PyMongo 函式庫 皆為 MongoDB 官方提供，目前只支援 Python3\npymongo[srv] 讓 Python 連線至 Atlas\n$ pip install pymongo $ pip install \u0026#34;pymongo[srv]\u0026#34; 存取空氣品質指標(AQI)資料 新增資料 政府資料開放平臺 免費提供各式各樣的資料集 (Open Data)，涵蓋生育保健、開創事業、購屋遷徙等多種議題，對數據分析人員而言這真是挖寶的好地方，此次以 空氣品質指標(AQI) 為資料來源，分析近期台灣空汙狀況 ~\nP.S. 此處未將各空汙指標轉為數字型態再存進資料庫，後續將以 MongoDB 語法處理\nReference：政府資料開放平臺\n查詢資料 空氣品質指標(AQI)資料中有兩個欄位名稱 (pm2.5 和 2.5_avg) 含 ($) 和 (.)，雖然 MongoDB v6.0 官方說明文檔顯示在 v5.0 後的版本支持字段名稱含 ($) 和 (.)，但實測仍然有問題\n無法解析帶有 (.) 的 field，因此需在匯入 MongoDB 前，先將 pm2.5 和 2.5_avg 更名為 pm2_5 和 pm2_5_avg\nMongoDB 透過正規表示法設定多樣化的查詢條件：\nregex 運算子 {\u0026lt;field\u0026gt;:{\u0026#39;$regex\u0026#39;: \u0026#39;pattern\u0026#39;, \u0026#39;$options\u0026#39;: \u0026lt;options\u0026gt;}} 正則表達式對象 {\u0026lt;field\u0026gt;: /pattern/\u0026lt;options\u0026gt;} 欲查詢彰、雲、嘉地區觀測站 pm2.5 指數，利用包含運算子 $in，查詢符合陣列中元素的資料，因地區可能包含縣、市，這裡想結合模糊查詢實現查詢\n$in 只能使用正則表達式對象，以下指令可於 mongosh 執行，但在 python 環境不適用（因陣列中的字串未加引號會報錯）：\n{\u0026#39;county\u0026#39;:{\u0026#39;$in\u0026#39;:[/彰化/, /雲林/, /嘉義/]}} pymongo 提供兩種方法：\n$regex 對單個關鍵詞進行模糊查詢 {\u0026#39;$regex\u0026#39;:\u0026#39;彰化\u0026#39;} re.compile() 對多個關鍵詞進行模糊查詢 {\u0026#39;$in\u0026#39;:[re.compile(\u0026#39;彰化\u0026#39;), re.compile(\u0026#39;雲林\u0026#39;), re.compile(\u0026#39;嘉義\u0026#39;)]} 運用 where 語句 欲查詢 AQI 值大於 50 小於 100 的資料，但 AQI 欄位型態為 String 無法做比較運算，因此可透過 pymongo 提供的特殊函數 where() 函數配合 JavaScript 的型別轉換函數實現\nReference：Install MongoDB Community Edition on Windows\nReference：Field Names with Periods (.) and Dollar Signs ($)\nReference：朱克剛(2022)。MongoDB 5.x實戰應用\n","permalink":"https://uuuna2233.github.io/blog/mongodb-in-python/","tags":["Python","MongoDB"],"title":"MongoDB 存取全台空氣品質指標 Part 1"},{"categories":["Project"],"contents":" 員工的離職原因林林總總，只有兩點最真實：1 錢，沒給到位 2 心，委屈了\n本篇主要講述自動化排程爬蟲程式和服務上雲，並產出視覺化分析讓求職者參考，此專題 Part 1 傳送門\n大數據相關職缺分析專題 Part 1 專題目標 - 讓帶著一顆焦慮的心求職的我們都能順利轉職 透過此次大數據相關的職缺分析，更清晰我們在就業市場的定位，並能根據這半年在養成班的所學，優先精進某幾項核心競爭力，明確自己的求職目標\n大型求職網站的職缺現狀為何？相關資力與薪資級距的關係？有數據人才需求的公司樣態？哪些技能或工具是多數公司的必備條件？\n資料流程 - 蒐集 → 處理 → 儲存 → 調整 → 分析 → 視覺化 處理完資料缺漏、分類、離群值和統一格式後，匯入 pymysql 或 SQLAlchemy 模組並設定連線參數，將資料以 dataframe 形式寫入關聯式資料庫 MySQL\n如何完成 Data Pipeline 自動化排程？ ● Linux 系統由 cron 來控制例行性工作排程，本次專題則以 crontab 指令建立爬蟲程式排程，依據不同網站的更新頻率和職缺數量，排定不同更新週期\n● 資料庫裡的職缺也需定期清理，若該職缺更新日期超過 30 天前，則設定 Event Scheduler 事件排程器執行刪除任務\nP.S. 設定完成後，可下指令即時監測 CRON 是否如期運作 tail -f /var/log/syslog | grep CRON 將服務佈署至雲端 於 AWS 雲端平台架設執行個體 EC2，本次作業系統選擇 Ubuntu\n於雲端虛擬主機中安裝 Anaconda 編輯器執行 Python 命令\n於 AWS 雲端平台使用資料庫服務 RDS，本次資料庫引擎選擇 MySQL\n安裝 GUI 應用程式 MySQL-Workbench 以連線到雲端資料庫\n使用 Lambda 排程自動開啟/停止執行個體，以控制使用量節省成本\n先建立 IAM 政策和執行角色，再編寫 Lambda 函數並佈署，最後選取觸發 Lambda 函數的 EventBridge 規則\n製作可視化分析報表 再將資料匯入 Tableau 前，需進行資料預處理，如：希望呈現不同技能點對應的職缺類型、數量和薪資\nSTEP1 拆分 skill 欄位，將不同技能分為多欄\nSTEP2 將欄索引旋轉為列索引，並置於列索引最內層\nSTEP3 不保留層級為 1 的索引，並加回原 Dataframe\n以 6/24 為基準，收集 30 日內大數據相關職缺，104 和 1111 人力銀行為職缺刊登大宗，且每日更新的職缺數超過 10,000 筆，顯示企業攬才動能高\n整體而言，職位要求技能前 5 名為 sql、python、linux、git、api，此為應扎好基本功的必備技能\n若求職者有意朝〔機器學習工程師〕目標邁進，則可參考以下技能需求持續精進\nP.S. 此次分析因限定在養成班所學技能，所能呈現之技能、工具種類有限\n整體而言，職缺具有一定的薪資水準（月薪 40,000 元起）\n若單獨查看與養成班所學最密切相關的〔數據分析師〕\u0026amp;〔數據工程師〕職缺，其 0-2 年年資也有不錯的薪資行情\n大數據領域發展日新月異，觀念、技術、算法快速迭代，隨時保持一顆探尋答案的好奇心，和市場演進與時俱進 ٩(^ᴗ^)۶\n連結至 網頁儀表板 動態呈現分析結果 完整程式碼請至 Github ","permalink":"https://uuuna2233.github.io/blog/bigdata-job-part2/","tags":["Python","AWS","MySQL","Tableau"],"title":"大數據相關職缺分析專題 Part 2"},{"categories":["Project"],"contents":" 員工的離職原因林林總總，只有兩點最真實：1 錢，沒給到位 2 心，委屈了\n用戶畫像 - 養成班的我們對未來的期待與迷惘 ● 基本資訊：18-29歲 初入職場或轉職者\n● 特徵：學了各種數據分析技能，不停的探索著職涯地圖的我們\n● 痛點：不確定就業市場的用人需求，和自己需要鎖定哪項技能專精\n專題目標 - 讓帶著一顆焦慮的心求職的我們都能順利轉職 透過此次大數據相關的職缺分析，更清晰我們在就業市場的定位，並能根據這半年在養成班的所學，優先精進某幾項核心競爭力，明確自己的求職目標\n大型求職網站的職缺現狀為何？相關資力與薪資級距的關係？有數據人才需求的公司樣態？哪些技能或工具是多數公司的必備條件？\n資料流程 - 蒐集 → 處理 → 儲存 → 調整 → 分析 → 視覺化 至 104、1111、518、YOURATOR、CAKERWSUME 爬取與數據分析相關職缺，將搜尋關鍵詞定為以下 8 大職務類別：\n數據分析師 商業分析師 數據工程師 機器學習工程師\n數據庫工程師 研究人員 軟體開發工程師 維運工程師\n如何篩選出與我們所學相符的職缺？ 運用 Python 中正規表達式(Regular Expression)的模組 re，清洗出職位要求限制包含所學技能的職缺\n如何歸納職務類別？ 需準備一個寫好職務名稱的字典集（相同職務類別不同公司可能會有不同 job title），運用 Python 中字符串模糊匹配庫 FuzzyWuzzy，依據 Levenshtein Distance 算法，計算該 job title 與字典集字詞之間的編輯距離，找出相似度最高的字詞並歸類\n如何計算薪資級距？ 薪資計算方式不同(時薪、日薪、月薪、年薪、按件計酬)且各網站格式不一，需以判斷式先做一輪清洗，又未寫明薪資的職缺數高達 50%，若將面議的薪資數值全數填為 40,000 顯然不合理，因此將資料庫裡寫明薪資的職缺，剔除離群值後，回填最高最低薪之 50 百分位數\n完整程式碼請至 Github ","permalink":"https://uuuna2233.github.io/blog/bigdata-job-part1/","tags":["Python","AWS","MySQL","Tableau"],"title":"大數據相關職缺分析專題 Part 1"},{"categories":["Post"],"contents":"Splunk 基於 MapReduce 分散式架構來處理大量資料，可收集、索引、分析各種來源（如：應用程序、伺服器和設備）生成的實時和歷史數據，且所存取的資料無需正規化（normalize）處理\n於 Linux 安裝 Splunk Reference：Splunk Installation Manual\n測試設備 Log 是否正常監聽 變更 Windows 系統內之事件（如：新增、刪除使用者、修改事件紀錄檔），再到 Splunk 上查看事件是否順利傳送與接收\n建⽴多台電腦之事件監控 本次串聯超過 50 台電腦設備，Splunk 執行個體接收來自轉送器所傳入的其他主機資料，收集其電腦事件紀錄檔 (Log)，藉此監控此集群架構內的資安狀況\n機器資料凡走過必留下痕跡，透過日誌管理、資安稽核、事件監控分析與告警，可預防資安危機，也能對系統狀態（如：效能、異常狀況）監控與故障排除，進行資源規劃以達節流之效\n","permalink":"https://uuuna2233.github.io/blog/splunk-monitoring/","tags":["Splunk"],"title":"Splunk 集中化監控不同主機事件"},{"categories":["Post"],"contents":"Photo by Bridgera\n● 靜態網頁爬蟲：client 向 server 發出 request，server 將網頁文件返回 response，瀏覽器解讀 HTML 並顯示結果，使用單純的 requests 函式庫即可爬取資料\n● 動態網頁爬蟲：client 向 server 發出 request，server 到 Database 存取資料並返回 response，使用 Selenium 套件模擬瀏覽器擷取資料\n透過 API (Application Programming Interface) 實踐 Web API 的資料傳輸格式多為 JSON 與 XML，格式漂亮的 JSON 檔可直接使用，而 XML 則需以 BeautifulSoup、lxml 完成解析\n範例：OMDb API為開放的電影數據庫，基於 RESTful 架構的服務，用於獲取電影訊息\n範例：民報、鉅亨網、自由時報、104薪資情報 等皆可透過開發者工具查看 Network，解析其 API 的參數規律\n使用 GET 的方式 使用 POST 的方式 靜態網頁爬取 與透過 API 爬取方式類似，只是 API 是網站工程師包好特定資料返回，自己以 requests 送出請求是返回整個網頁內容，需再進一步解析 HTML 結構並對其標籤定位\n範例：PTT、TechNews、奇摩股市、中國時報 等皆可透過開發者工具查看 Elements，解析其 HTML 結構\n動態網頁爬取 網頁以滾動捲軸或點擊特殊按鍵來動態載入更多的資料，此時就需以 Selenium 套件，模仿人類行為處理網頁操作\n範例：104人力銀行 職缺搜尋，捲軸滑到最底部會自動載入下一頁，第 16 頁後則需手動點擊載入\n更新：DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead. 訊息指出 find_element_by_* 命令已被棄用，driver.find_element(By.CLASS_NAME, \u0026ldquo;XXXXX\u0026rdquo;)\nReference：Selenium Locating Elements\n完整程式碼請至 Github ","permalink":"https://uuuna2233.github.io/blog/web-crawling-in-python/","tags":["Python"],"title":"Python 網頁爬蟲 (Web Crawler) 實踐"}]