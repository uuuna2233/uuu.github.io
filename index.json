[{"categories":["Note"],"contents":" Objectives 1. Explain the value of the AWS Cloud (雲的價值)\n2. Understand and explain the AWS shared responsibility model (責任共擔模型)\n3. Understand security best practices (安全最佳實踐)\n4. Understand AWS Cloud costs, economics, and billing practices (雲的成本、經濟性與計費)\n5. Describe and position the core AWS services, including compute, network, databases, and storage (AWS核心服務)\n6. Identify AWS services for common use cases (使用案例)\nModule 1: Cloud Concepts Benefits of the AWS Cloud\n◆ Trade fixed expense for variable expense (Instead of investing heavily in data centers and servers before using them) ◆ Economy of scale (Offer the services at a low cost to the consumer) ◆ Elasticity (Horizontally scale EC2, scaling up and down as required with only a few minutes’ notice) ◆ High availability (Resiliency in architecture and avoiding single points of failure) ◆ Increase speed and agility (Add and remove new or existing services quickly and easily, and new IT resources are only a click away) Q: How the AWS Cloud allows users to focus on business value? A: Focus on projects that differentiate your business, not the infrastructure. Cloud computing lets you focus on your own customers, rather than on the heavy lifting of racking, stacking, and powering servers. Aspects of AWS Cloud Economics\n◆ Cost Savings (TCO): Infrastructure cost savings/ avoidance from moving to the cloud. ◆ Staff Productivity: Efficiency improvement by function on a task-by-task basis. ◆ Operational Resilience: Benefit of improved availability, security, and compliance. ◆ Business Agility: Deploying new features/ applications faster and reducing errors ◆ AWS pricing calculator (Cost estimate): Helps with calculating the total cost of ownership. ◆ Migration Evaluator: Helps with inventorying existing environment, identifying workload information, and designing and planning AWS migration. ◆ Cost Explorer: Rightsizing recommendations analyze your Amazon EC2 resources and usage to show opportunities for how you can lower your spending. Cloud Architecture Design Principles\n◆ Design for failure: Understanding what and how components fail, and how to architect around failures to add resiliency. (Using at least two servers) ◆ Elasticity: is the ability of a system to grow to handle increased load. 1. Vertical scaling- increase in the specifications of an individual resource 2. Horizontal scaling- increase in the number of resources Q: How can this web application be optimized by just looking at different storage options? A: 1. Amazon ElastiCache is a fully managed, in-memory caching service supporting flexible, real-time use cases. You can use ElastiCache for caching, which accelerates application and database performance, or as a primary data store that don't require durability. 2. Amazon CloudFront is a content delivery network (CDN) service built for high performance, security, and decrease end-user latency. CloudFront speeds up content delivery by leveraging edge locations, to reduce delivery time by caching your content close to your end users. ◆ Decouple components (解耦組件) vs. monolithic architecture (單體架構)： Monolithic architecture- all processes are tightly coupled or connected, and run as a single service. Decouple components- decoupled application components, then each would gain the ability to be scaled and managed based on individual needs. ◆ Implement elasticity in the cloud vs. on-premises: Dynamically change capacity based on demand in cloud (Elasticity、Scalable). Saving time during setup and removes the undifferentiated heavy lifting. ◆ Think parallel: Serial and sequential processing are limiting, dependencies can make or break entire processes. Looking at how can divide a job into its simplest form, and then distribute that load to multiple components to handle the demand. Multi-threading requests by using multiple concurrent threads will store or fetch the data faster than requesting it sequentially. Distribute the incoming requests across multiple asynchronous web servers using a load balancer laaS, PaaS, and SaaS\n◆ Infrastructure as a Service (IaaS) 基礎設施即服務 (EC2、Lightsail、VPC、Direct Connect) ◆ Platform as a Service (PaaS) 平台即服務 (S3、Deep Learning AMI、Cloud Watch、Elastic Beanstalk) ◆ Software as a Service (SaaS) 軟體即服務 (RDS、DynamoDB、Redshift、Glue、EMR、Kinesis) ◆ Infrastructure as code (IaC) 基礎設施即代碼- creation, deployment, and maintenance of infrastructure in a programmatic, descriptive, and declarative way. AWS CDK\nAWS Cloud Formation Reference：\n1. AWS Fundamentals- Core Concepts\n2. Business Value on AWS\n3. Cloud Economics Center\n4. AWS Pricing/TCO Tools\n5. Optimizing your cost with Rightsizing Recommendations\n6. Design Principles\n7. Architecure Best Practices\n8. Common ElastiCache Use Cases and How ElastiCache Can Help\nModule 2: Security and Compliance AWS Shared Responsibility Model (https://aws.amazon.com/tw/compliance/shared-responsibility-model/)\n◆ customer’s or AWS responsibility? The level of responsiblity the customer assumes changes depending on the service they are using. ex: AWS is responsible for patching on Amazon RDS、underlying/networkinfrastucture (software 底層、網路基礎設施)、Physical secuity( hardware 全球基礎設施), customer are responsible for patching DB running on Amazon EC2, depends on if an AWS service is managed or not (是否託管服務) AWS Cloud Security and Compliance Concepts (https://aws.amazon.com/tw/compliance/programs/)\n◆ AWS Artifact: provides on-demand access to AWS’ security and compliance reports and select online agreements. ◆ Data Encryption: 1. Data at rest encryption capabilities available in most AWS services. 2. Dedicated, hardware-based cryptographic key storage using AWS CloudHSM 3. Encrypted message queues for the transmission of sensitive data using server-side encryption (SSE) for Amazon SQS ◆ AWS Cloudwatch: is a monitoring service that monitoring data generated by resources, ex: give insight into if an overloaded EC2 or an unusually high number of requests hitting an Elastic Load Balancer. ◆ AWS CloudTrail: is a service that logs AWS API calls, enables risk auditiong by continuously monitoring and logging account activity, including user actions in the AWS Management Console and AWS SDKs. (紀錄帳戶所有活動) ◆ AWS Config: is a service that can use to assess, audit, and evaluate the configurations of AWS resources. (只記錄對資源配置的更改) AWS Access Management Capabilities (https://docs.aws.amazon.com/IAM/latest/UserGuide/id.html)\n◆ Least privilege: Only give people exactly the level of access that they need. ◆ Root user: has complete and unrestricted access to all resources in an AWS account, should not be using this user to carry out daily tasks in AWS. Use multi-factor authentication (MFA) for locking away root user credentials, rotating access keys and the password to protect it. ◆ IAM: Uses/ Groups/ Roles/ Policies 1. Uses: Give people the ability to sign in to the AWS Management Console for interactive tasks and to make programmatic requests to AWS services using the API or CLI. 2. Groups: Attach policies to multiple users at one time. 3. Roles: A role does not have any credentials (password or access keys) associated with it. Instead of being uniquely associated with one person, a role is intended to be assumable by anyone who needs it. 4. Policies: Manage access in AWS, attaching them to IAM identities (users, groups of users, or roles) or AWS resources. Q: When to create an IAM role (instead of a user)? A: 1. You're creating an application that runs on an EC2 instance and that application makes requests to AWS. 2. You're creating an app that runs on a mobile phone and that makes requests to AWS. 3. Users in your company are authenticated in your corporate network and want to be able to use AWS without having to sign in again—that is, you want to allow users to federate into AWS. The following flow chart provides details about how the decision is made. This flow chart does not cover the impact of resource-based policies and implicit denies in other types of policies.\nNetwork Security (https://aws.amazon.com/products/security/network-application-protection/)\n◆ Security groups: are instance-level firewalls, they can allow a request based on port, protocol, and source or destination, but cannot inspect an HTTP packet. ◆ Network access control list (network ACLs): are subnet-level firewalls, they can allow or deny traffic based on traffic type, port, protocol and source or destination, but cannot inspect request's contents. ◆ AWS web application firewall (WAF): is a firwall that can fliter out traffic based on any part of the request, such as IP addresses, HTTP headers, HTTP body, or URL strings. ex: it can filter out any requests that have SQL code, which would prevent a SQL injection attack. ◆ AWS Trusted Advisor (監控服務配額以降低成本、提升效能並改善安全)/ Amazon Inspector (自動化漏洞管理服務，可不斷掃描 AWS 工作負載以尋找軟體漏洞和意外網路風險): Can give recommendations around security ◆ AWS Marketplace: Find third-party security software Reference：\n1. Shared Responsibility Model\n2. AWS Artifact\n3. Security, Identity, and Compliance on AWS\n4. AWS Identity and Access Management\n5. Security best practices in IAM\n6. AWS CloudTrail\n7. Amazon CloudWatch\n8. AWS Config\n9. Security best practices for your VPC\n10. Control traffic to resources using security groups\n11. Control traffic to subnets using Network ACLs\n12.. IPv4 \u0026amp; Subnet Mask\nModule 3: Technology Methods of Deploying and Operating in the AWS Cloud\n◆ Methods to communicate to the AWS Cloud: 1. APLs and SDKs 2. AWS Command Lind Interface (CLI) (ex: Linux shell、AWS CloudShell) 3. AWS Management Console 4. Infrastructure as code (laC) 5. Integrated Development Environments (IDE) ◆ Amazon VPC 1. Network-to-Amazon VPC 2. Amazon VPC-to-Amazon VPC 3. Software remote access-to-Amazon VPC 4. Transit VPC Q: Which components are requires to build a successful site-to-site VPN connection on AWS? A: Customer Gateway、Virtual Private Gateway AWS Global Infrastructure\n◆ Availability Zones: are one or more discrete data centers with redundant power, networking, and connectivity in an AWS Region. ◆ Regions: are phyical location that consists of clusters of data centers. There are many different Regions around the world, and enable the deployment of compute and storage resources globally. ◆ Edge Locations: Endpoints that serve cached content and provide access to AWS service. Core AWS Services\n◆ AWS Compute: ◎ Instances 1. EC2- [Instance Types](https://aws.amazon.com/ec2/instance-types/)、[Pricing](https://aws.amazon.com/ec2/pricing/) 2. AWS Batch- Run hundreds of thousands of batch computing jobs on AWS. Q: What are the differences between Amazon EC2 and Amazon Lightsail? A: Amazon EC2 instances are meant for small to complex architecture. Lightsail is better for small to medium scale workloads. (https://aws.amazon.com/tw/premiumsupport/knowledge-center/lightsail-differences-from-ec2/) ◎ Containers 1. Amazon ECS- Container orchestration/ management service. 2. AWS Fargate- Run containers without having to manage servers or clusters. 3. Amazon ECR- Fully-managed Docker container registry that manage Docker container images. 4. Amazon EKS- Run the Kubernetes management infrastructure across multi-AZ to eliminate a single point of failure. 5. AWS App Runner- is the easiest way to run web application (including API services, backend web services, and websites) on AWS. Q: AWS EKS vs. ECS vs. Fargate vs. Kops? A: https://cast.ai/blog/aws-eks-vs-ecs-vs-fargate-where-to-manage-your-kubernetes/ ◎ Serverless 1. AWS Lambda- Run code without provisioning or managing servers, and set up code to automatically trigger from other AWS services. ◎ Edge and hybrid 1. AWS Outposts- Run AWS infrastructure and services on premises. 2. AWS Snow Family- Move petabytes of data to and from AWS, or process data at the edge. 3. AWS Wavelength- Embeds AWS compute and storage services within 5G networks, providing mobile edge computing infrastructure for ultra-low-latency applications. ◎ Cost and capacity management 1. AWS Elastic Beanstalk- Simply upload code, and AWS Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto scaling to application health monitoring. 2. EC2 Image Builder- Keeping virtual machine (VM) and container images up-to-date ◆ AWS Storage: 1. Amazon Elastic Block Store (EBS)- Provides persistent block storage volumes and automatically replicated within its Availability Zone. 2. Amazon Elastic File System (EFS)- provides a simple, scalable, elastic file system for Linux-based workloads for use with AWS Cloud services and on-premises resources. 3. Amazon Simple Storage Service (S3)- is an object storage service. storage classes: S3 Intelligent-Tiering/ S3 Standard/ S3 Standard-IA/ S3 Glacier 4. AWS Storage Gateway- is a hybrid storage service that allows on-premises applications to seamlessly use AWS cloud storage. The gateway connects to AWS storage services. Q: When should I use Amazon EFS vs. Amazon EBS vs. Amazon S3? A: S3 is for object storage.(photos, videos, files, and simple web pages) EBS is for EC2 block storage.(computer’s hard drive) EFS is a file system for many EC2 instances.(multiple EC2 instances and lots of data) ◆ Networking and Content Delivery: 1. Amazon API Gateway- Create an API that acts as a “front door” for applications to access data, business logic, or functionality from back-end services. 2. Amazon CloudFront- Securely deliver content with low latency and high transfer speeds. 3. Amazon Route 53- Domain Name System (DNS) web service. 4. Amazon VPC- Provides a logically isolated section of the AWS Cloud where you can launch AWS resources in a virtual network. 5. AWS Direct Connect- Establishes a dedicated network connection from premises to AWS. 6. AWS Global Accelerator- Improves application availability by continuously monitoring the health of your application endpoints and routing traffic to the closest healthy endpoints. 7. AWS Transit Gateway- Connects VPCs and on-premises networks through a central hub. ◆ Database: 1. Amazon Aurora- is a MySQL and PostgreSQL compatible relational database engine 2. Amazon DynamoDB- is a key-value and document database 3. Amazon ElastiCache- Retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower disk-based databases. Caching engines: Redis/ Memcached AWS Technology Support\nSupport areas: 1. Documentation (Whitepapers, AWS Knowledge Center, Forums, Blogs) 2. Account-specific support 3. AWS Partner Network (APN) and AWS Marketplace 4. AWS Trusted Advisor Q: Which AWS support plan provides access to architectural and operational reviews, as well as 24/7 access to senior cloud support engineers through email, online chat, and phone? A: Enterprise Reference：\n1. Tools to Build on AWS\n2. Global Infrastructure\n3. Amazon Virtual Private Cloud Connectivity Options\n4. Amazon Web Services Cloud\n5. 10個Q\u0026amp;A快速認識Docker\n6. 剖析容器的資安風險與防護\n7. AWS ECS 容器编排服务应用开发中文入门教学\n8. 比較Amazon EBS. EFS. S3：為您的企業選擇最佳的AWS存儲服務\n9. Amazon S3 vs EBS vs EFS\n10. Compare AWS Support Plans\nModule 4: Billing and Pricing Pricing Models\nQ: A company has an application that only needs to run for 2 hours AT ANY TIME during a day. Which EC2 type will be MOST cost-effective for this application? A: Spot Instances Various Account Structures\nAWS Organizations: 1. Centralized management of all of your AWS accounts. 2. Consolidated billing for all member accounts Identify resources available for billing support\n◆ AWS Cost Explorer: is an interface located in the AWS Mangement Console, and gives a way to visualize and manage costs in a granular way. (by month, service, tag...) ◆ AWS Cost and Usage Report: contains the most comprehensive set of AWS cost and usage data available. ◆ Amazon QuickSight: is a Data-visualization tool to analyze AWS usage and costs, or to create custom reports. ◆ AWS Simple Monthly Calculator: Estimate how much bill will be based on usage projections. ◆ AWS Budgets: Alerted by email or SNS notification when actual or forecasted cost and usage exceed your budget threshold, or when your actual RI and Savings Plans' utilization or coverage drops below your desired threshold. Reference：\n1. What is AWS Organizations?\n2. Instance purchasing options\n3. AWS Certified Cloud Practitioner\n","permalink":"https://uuuna2233.github.io/blog/aws-practitioner/","tags":["AWS","Certification"],"title":"AWS CCP Learning Note"},{"categories":["Project"],"contents":" 專案目標 以 Python 處理日誌資料，並串接 Google Sheets API 將結果自動導入雲端儲存，再將分析結果以 Power BI 視覺化呈現，協助主辦方進行賽事統計與提升伺服器效能\nGoogle Cloud 1. 先到 GCP 建立專案吧！( https://console.cloud.google.com/ )\n2. 再啟用 Google Sheets API\n3. 建立憑證和服務帳戶金鑰\n4. 共用 Google 試算表予服務帳戶，輸入電子郵件並設定編輯者權限\nGoogle sheets API 查看 Google Sheets for Developers，發現每分鐘可調用次數蠻多的，每日讀寫次數則無限制\nBut! 我的帳戶還是被停權了 (ಥ﹏ಥ)，可能是服務帳戶的 API 調用被視為單個帳戶使用，因此即使沒有超出上述使用限制，也無 Response 429: Too many requests，在團隊有多人需要將資料打進同個 Google 試算表的情況下，會需要 Request a higher quota limit! ( https://developers.google.com/sheets/api )\n連結 Google 試算表與設定要使用的工作簿 1. 安裝相關模組\npip install gspread gspread-dataframe oauth2client gspread-formatting 2. 設定操作憑證與範圍 3. 連結試算表並新建當日 sheet 以 Python 清洗賽事、伺服器 log 檔 玩家生存戰況 (Game Log) log 格式 → [19:01:59] [Server thread/INFO]: XXXXXXXX was slain by YYYYYYYY 寫入 Google Sheet 概覽 玩家成就達成 (Advancement Log) log 格式 → [18:43:55] [Server thread/INFO]: XXXXXXXX has made the advancement [Monster Hunter] 伺服器過載紀錄 (Server Log) log 格式 → [18:43:04] [Server thread/WARN]: Can\u0026#39;t keep up! Is the server overloaded? Running 2011ms or 40 ticks behind 寫入 Google Sheet 概覽 防火牆阻擋紀錄 (Ufw Log) log 格式 → Jul 29 15:58:45 mineos-tkldev kernel: [19012.171669] [UFW BLOCK] IN=ztuze7nxnt OUT= MAC=8e:5b:02:67:d9:22:8e:05:bd:df:16:d0:08:00 SRC=172.23.86.214 DST=172.23.249.234 LEN=40 TOS=0x00 PREC=0x00 TTL=64 ID=53307 PROTO=TCP SPT=62711 DPT=8443 WINDOW=4096 RES=0x00 ACK FIN URGP=0 P.S. 系統配發的 id 固定不變，而玩家自定義的 id 可更動\nlog 格式 → [19:21:15] [User Authenticator #25/INFO]: UUID of player XXXXXXXX is c93a7158-d0dd-4d95-85f3-d79ab5155f60 log 格式 → [19:21:15] [Server thread/INFO]: XXXXXXXX[/1.34.43.184:43209] logged in with entity id 17171 at (-151.14292647891168, 78.00430070304934, 159.96209798626114) 寫入 Google Sheet 概覽 計算同時在線玩家人數 (Joined Log) log 格式 → 2022-07-26 [13:26:38] [Server thread/INFO]: XXXXXXXX joined the game 寫入 Google Sheet 概覽\n以 Power BI 視覺化解析賽事結果 Reference：\n1. Google Sheets for Developers\n2. Examples of gspread Usage\n3. Python大數據特訓班(第二版)\n完整程式碼請至 Github ","permalink":"https://uuuna2233.github.io/blog/minecraft-log/","tags":["Python","Power BI"],"title":"Minecraft 麥塊競賽 Log 解析"},{"categories":["Project"],"contents":" 業務場景 AWS 擁有複雜度超高的計費方式，帳單報告多達 150 個欄位，不同的雲服務、產品規格、用量狀況、計價模式、生效時間等等讓人眼花撩亂 (◎.◎)\n此次專案目標為協助雲端平台服務商從無到有建立會員帳務系統，讓 客戶可以清楚地找到關注的訊息 (ex 每日成本用量，折扣使用狀況)，也讓企業能動態更新帳務，將此業務計算自動化，減少在此投入的人工，進而 專注於雲端整合服務的本業上\n雲端架構 ● CUR：內含最完整的 AWS 成本和用量數據，依產品代碼、項目類型、使用時間逐項列出帳戶或組織層級的用量\n● S3：CUR 會向 S3 bucket 提交以小時細分的帳務報告，每天更新一次，於次月第 7 日提供最終報告\n● Lambda：以 trigger 接收 S3 Event，當有新帳單存入 S3，即啟動相對應的 Funtion，對帳單做一系列的處理並存入 DB\n● CloudWatch：監控 Lambda Funtion 運行狀況，並為 LOG 設定警示條件，在異常時透過 Alarm 通知 Admin\n● MySQL - 開發階段在 EC2 自建 DB 以節省成本，正式上線可考量採用 AWS RDS 服務建立 MySQL\n● Elastic IP：為針對動態雲端運算設計的靜態 IPv4 地址，關聯至指定的 EC2，公有 IP 即不會異動\n● VPC：為快速進行開發，目前配有一個 Public Subnet，正式上線需將服務拆分不同 Instance 並置於 Private Subnet\n如何查看最新報告? S3 可以找到 Manifest.json 檔案，清單紀錄了報告的架構，reportKeys 為最新報告文件的 S3 文件路徑\n如何確定最終報告? 如果是最終版，bill/InvoiceId 會填入 \u0026lsquo;AWS invoice ID\u0026rsquo;，bill/BillType 則填入 \u0026lsquo;Anniversary\u0026rsquo; 清洗入庫效能比較 因業務需求，帳單需拆分為 Table billing_1、Table billing_2\n先判斷 A 欄位的類型，再判斷 B 欄位和 C 欄位一致與否\n● Option 1：使用迴圈讀取並逐條判斷，處理效能低落，每秒約處理 2 列，需耗時 21 小時\n● Option 2：使用 dataframe 一次處理，處理效能明顯優於前者，15 萬條數據 10 秒內完成入庫\n資料庫架構 設計完整的資料庫系統，要考量的細節很多，圍繞在以下列舉的考量，皆非常考驗著業務理解和熟悉資料庫背後工作原理\u0026hellip;\n應用業務的需求上： ex 客戶關心什麼內容 / 前端頁面呈現邏輯 / 模擬內部作業流程（如：團隊在 Key in 客戶訊息、折扣等內容時夠不夠直覺好用） 資料庫效能與容量： ex 前端如何透過 API 向後端請求資料 / 資料存取類型與編碼 / 索引、外鍵如何設計 / 應累加或定期更新哪些資訊 後續維護和需求延展：ex 使用者的權限管理 / 應依照客戶或是週期分庫分表 / 未來業務變化需更動表格配置時夠不夠彈性 資料庫正規化 (Normalization) 第一正規化 (1NF)：每個欄位只能有一個基元值 (Atomic)，表中有主鍵，而其它欄位都相依於主鍵\\ 第二正規化 (2NF)：每個非鍵屬性必須「完全相依」於主鍵，亦即將「部分功能相依」的欄位分割，再另外組成新 Table\\ 第三正規化 (3NF)：各欄位與主鍵之間沒有「遞移相依」的關係，亦即將「遞移相依」的欄位分割，再另外組成新 Table AWS 原始帳單已符合每個欄位僅有單一值，將 identity/LineItemId 和 identity/TimeInterval 當作聯合主鍵，product/sku 為產品的獨特代碼，可以此欄為作為另一張表的主鍵，利用「外鍵」來連接主表\n事件排程器 (Event Scheduler) delimiter $$ CREATE [DEFINER = user] EVENT [IF NOT EXISTS] event_name ON SCHEDULE schedule [ON COMPLETION [NOT] PRESERVE] [ENABLE | DISABLE | DISABLE ON SLAVE] [COMMENT \u0026#39;string\u0026#39;] DO event_body; end $$ delimiter; schedule: { AT timestamp [+ INTERVAL interval] ... | EVERY interval [STARTS timestamp [+ INTERVAL interval] ...] [ENDS timestamp [+ INTERVAL interval] ...] } interval: quantity {YEAR | QUARTER | MONTH | DAY | HOUR | MINUTE | WEEK | SECOND | YEAR_MONTH | DAY_HOUR | DAY_MINUTE | DAY_SECOND | HOUR_MINUTE | HOUR_SECOND | MINUTE_SECOND} SET GLOBAL event_scheduler = ON; identity/LineItemId 項目可能會依照不同的報告時間改變，因此需每日以新報告覆蓋原表格\n而以此表格延伸出的數張報表亦需每日更新，即需啟用 SQL 排程來完成 設計索引、外鍵 (Index、Foreign Key) 適當的索引可以加快數據訪問，提高查詢效能；而外鍵的約束用於防止非法數據插入，更新、刪除時也能保持表格一致性\n前端經常返回的變數為 \u0026lsquo;business_tax\u0026rsquo;，即經常使用 \u0026lsquo;business_tax\u0026rsquo; 執行條件判斷，即能為它建立索引\n若發現 query 速度很慢，在 query 敘述前使用 EXPLAIN，可以解析查詢狀況，進而調優 SQL 語法\n多階層子查詢、結合查詢 (Subquery、Join) 查詢如何撰寫會大幅度的影響效能，尤其在複雜的巢狀子查詢、多次結合查詢\n比如需求 JOIN 4 張表，是要 A JOIN B 後，再 JOIN C，最後 JOIN D，或者 A JOIN B、C JOIN D，再 JOIN 前述兩表\u0026hellip;\n該 WHERE A.ID 或者 WHERE B.ID，是否使用了暫時表格 (Temporary Table)\u0026hellip;\n資料庫是大數據從業人員必備技能，雖入門容易，但學習曲線到某一程度會大幅趨緩，需投入更多時間才能成長，關注的點也不僅在於「查詢的出正確結果」就好，未來得持續精進如災難復原、日誌監控、Cluster配置、SQL injection\u0026hellip;等知識\nReference：\n1. AWS Data dictionary\n2. MySQL Normalization\n3. MySQL Event Scheduler\n","permalink":"https://uuuna2233.github.io/blog/aws-billing-system/","tags":["Python","AWS","MySQL","Linux"],"title":"於 AWS 雲端平台搭建會員帳務系統"},{"categories":["Post"],"contents":"● 非關聯式資料庫 (NoSQL)\nMongoDB 是文件導向式資料庫，儲存由 key-value 配對而成的資料，無需事先定義資料型態，對於不同表徵的資料，都能快速存進資料庫\n● BSON 格式\n以 JSON 字串存進資料庫，MongoDB 內部會將 JSON 轉成 BSON，由 text 格式轉成 binary 格式，加快文件解析速度，亦能儲存非文字資料\n● _id 欄位\nMongoDB 會為每筆資料自動建立 _id 欄位，預設內容為 ObjectID，可視為主索引鍵，用於區分不同 Document，ObjectID 前 4 個 bytes 為 Document 產生時的時間戳記\nRDBMS MongoDB Remarks Database Database 資料庫 Table Collection 資料表/聚集 Row Document 資料/文件 Column Field 欄位/鍵名 Primary _id 主鍵/主索引 View View 視觀表 安裝 MongoDB on Windows MongoDB 可於本地安裝 Community Server，也能使用 Atlas 在 AWS、Azure 或 GCP 建立雲端託管服務\n本次選擇本地 (Windows) 下載 msi 安裝檔，預設執行檔會放在 C:\\Program Files\\MongoDB\\Server\\[version]\\bin，此次將資料庫存放到 D 槽\n將 Windows 服務裡的 MongoDB 項目改為手動（正式上線使用前，改成手動以免重開機後 MongoDB server 自動啟動)\n可以安裝 GUI 工具 compass 和 Shell 工具 mongosh，直接操作 MongoDB 打開命令提示字元並輸入 mongosh.exe 以連接 MongoDB，指令相當於 mongosh \u0026ldquo;mongodb://localhost:27017\u0026rdquo;\n安裝 PyMongo 函式庫 皆為 MongoDB 官方提供，目前只支援 Python3\npymongo[srv] 讓 Python 連線至 Atlas\n$ pip install pymongo $ pip install \u0026#34;pymongo[srv]\u0026#34; 存取空氣品質指標(AQI)資料 新增資料 政府資料開放平臺 免費提供各式各樣的資料集 (Open Data)，涵蓋生育保健、開創事業、購屋遷徙等多種議題，對數據分析人員而言這真是挖寶的好地方，此次以 空氣品質指標(AQI) 為資料來源，分析近期台灣空汙狀況 ~\nP.S. 此處未將各空汙指標轉為數字型態再存進資料庫，後續將以 MongoDB 語法處理\nReference：政府資料開放平臺\n查詢資料 空氣品質指標(AQI)資料中有兩個欄位名稱 (pm2.5 和 2.5_avg) 含 ($) 和 (.)，雖然 MongoDB v6.0 官方說明文檔顯示在 v5.0 後的版本支持字段名稱含 ($) 和 (.)，但實測仍然有問題\n無法解析帶有 (.) 的 field，因此需在匯入 MongoDB 前，先將 pm2.5 和 2.5_avg 更名為 pm2_5 和 pm2_5_avg\nMongoDB 透過正規表示法設定多樣化的查詢條件：\nregex 運算子 {\u0026lt;field\u0026gt;:{\u0026#39;$regex\u0026#39;: \u0026#39;pattern\u0026#39;, \u0026#39;$options\u0026#39;: \u0026lt;options\u0026gt;}} 正則表達式對象 {\u0026lt;field\u0026gt;: /pattern/\u0026lt;options\u0026gt;} 欲查詢彰、雲、嘉地區觀測站 pm2.5 指數，利用包含運算子 $in，查詢符合陣列中元素的資料，因地區可能包含縣、市，這裡想結合模糊查詢實現查詢\n$in 只能使用正則表達式對象，以下指令可於 mongosh 執行，但在 python 環境不適用（因陣列中的字串未加引號會報錯）：\n{\u0026#39;county\u0026#39;:{\u0026#39;$in\u0026#39;:[/彰化/, /雲林/, /嘉義/]}} pymongo 提供兩種方法：\n$regex 對單個關鍵詞進行模糊查詢 {\u0026#39;$regex\u0026#39;:\u0026#39;彰化\u0026#39;} re.compile() 對多個關鍵詞進行模糊查詢 {\u0026#39;$in\u0026#39;:[re.compile(\u0026#39;彰化\u0026#39;), re.compile(\u0026#39;雲林\u0026#39;), re.compile(\u0026#39;嘉義\u0026#39;)]} 運用 where 語句 欲查詢 AQI 值大於 50 小於 100 的資料，但 AQI 欄位型態為 String 無法做比較運算，因此可透過 pymongo 提供的特殊函數 where() 函數配合 JavaScript 的型別轉換函數實現\n修改資料 若想更新某一個監測站資料，但不確定資料庫是否存在此監測站，可透過 upsert Parameter：\nWhen true, update() either:\n● Creates a new document if no documents match the query\n● Updates a single document that matches the query.\nDefaults to false, which does not insert a new document when no match is found.\n刪除資料 將方才練習更新的資料刪除，可直接指定 \u0026lsquo;sitename\u0026rsquo;，也能藉由「刪除最新一筆」資料來實現\nReference：\n1. Install MongoDB Community Edition on Windows\n2. Field Names with Periods (.) and Dollar Signs ($)\n3. 朱克剛(2022)。MongoDB 5.x實戰應用\n完整程式碼請至 Github ","permalink":"https://uuuna2233.github.io/blog/mongodb-in-python-part1/","tags":["Python","MongoDB"],"title":"MongoDB 存取全台空氣品質指標 Part 1"},{"categories":["Project"],"contents":" 員工的離職原因林林總總，只有兩點最真實：1 錢，沒給到位 2 心，委屈了\n本篇主要講述自動化排程爬蟲程式和服務上雲，並產出視覺化分析讓求職者參考，此專題 Part 1 傳送門\n大數據相關職缺分析專題 Part 1 專題目標 - 讓帶著一顆焦慮的心求職的我們都能順利轉職 透過此次大數據相關的職缺分析，更清晰我們在就業市場的定位，並能根據這半年在養成班的所學，優先精進某幾項核心競爭力，明確自己的求職目標\n大型求職網站的職缺現狀為何？相關資力與薪資級距的關係？有數據人才需求的公司樣態？哪些技能或工具是多數公司的必備條件？\n資料流程 - 蒐集 → 處理 → 儲存 → 調整 → 分析 → 視覺化 處理完資料缺漏、分類、離群值和統一格式後，匯入 pymysql 或 SQLAlchemy 模組並設定連線參數，將資料以 dataframe 形式寫入關聯式資料庫 MySQL\n如何完成 Data Pipeline 自動化排程？ ● Linux 系統由 cron 來控制例行性工作排程，本次專題則以 crontab 指令建立爬蟲程式排程，依據不同網站的更新頻率和職缺數量，排定不同更新週期\n● 資料庫裡的職缺也需定期清理，若該職缺更新日期超過 30 天前，則設定 Event Scheduler 事件排程器執行刪除任務\nP.S. 設定完成後，可下指令即時監測 CRON 是否如期運作 tail -f /var/log/syslog | grep CRON 將服務佈署至雲端 於 AWS 雲端平台架設執行個體 EC2，本次作業系統選擇 Ubuntu\n於雲端虛擬主機中安裝 Anaconda 編輯器執行 Python 命令\n於 AWS 雲端平台使用資料庫服務 RDS，本次資料庫引擎選擇 MySQL\n安裝 GUI 應用程式 MySQL-Workbench 以連線到雲端資料庫\n使用 Lambda 排程自動開啟/停止執行個體，以控制使用量節省成本\n先建立 IAM 政策和執行角色，再編寫 Lambda 函數並佈署，最後選取觸發 Lambda 函數的 EventBridge 規則\n製作可視化分析報表 再將資料匯入 Tableau 前，需進行資料預處理，如：希望呈現不同技能點對應的職缺類型、數量和薪資\nSTEP1 拆分 skill 欄位，將不同技能分為多欄\nSTEP2 將欄索引旋轉為列索引，並置於列索引最內層\nSTEP3 不保留層級為 1 的索引，並加回原 Dataframe\n以 6/24 為基準，收集 30 日內大數據相關職缺，104 和 1111 人力銀行為職缺刊登大宗，且每日更新的職缺數超過 10,000 筆，顯示企業攬才動能高\n整體而言，職位要求技能前 5 名為 sql、python、linux、git、api，此為應扎好基本功的必備技能\n若求職者有意朝〔機器學習工程師〕目標邁進，則可參考以下技能需求持續精進\nP.S. 此次分析因限定在養成班所學技能，所能呈現之技能、工具種類有限\n整體而言，職缺具有一定的薪資水準（月薪 40,000 元起）\n若單獨查看與養成班所學最密切相關的〔數據分析師〕\u0026amp;〔數據工程師〕職缺，其 0-2 年年資也有不錯的薪資行情\n大數據領域發展日新月異，觀念、技術、算法快速迭代，隨時保持一顆探尋答案的好奇心，和市場演進與時俱進 ٩(^ᴗ^)۶\n連結至 網頁儀表板 動態呈現分析結果 完整程式碼請至 Github ","permalink":"https://uuuna2233.github.io/blog/bigdata-job-part2/","tags":["Python","AWS","MySQL","Tableau","Linux"],"title":"大數據相關職缺分析專題 Part 2"},{"categories":["Project"],"contents":" 員工的離職原因林林總總，只有兩點最真實：1 錢，沒給到位 2 心，委屈了\n用戶畫像 - 養成班的我們對未來的期待與迷惘 ● 基本資訊：18-29歲 初入職場或轉職者\n● 特徵：學了各種數據分析技能，不停的探索著職涯地圖的我們\n● 痛點：不確定就業市場的用人需求，和自己需要鎖定哪項技能專精\n專題目標 - 讓帶著一顆焦慮的心求職的我們都能順利轉職 透過此次大數據相關的職缺分析，更清晰我們在就業市場的定位，並能根據這半年在養成班的所學，優先精進某幾項核心競爭力，明確自己的求職目標\n大型求職網站的職缺現狀為何？相關資力與薪資級距的關係？有數據人才需求的公司樣態？哪些技能或工具是多數公司的必備條件？\n資料流程 - 蒐集 → 處理 → 儲存 → 調整 → 分析 → 視覺化 至 104、1111、518、YOURATOR、CAKERWSUME 爬取與數據分析相關職缺，以 104 為例，我們可以使用 Selenium 滑鼠滾輪下滑至最底和自動點擊下一頁的功能抓取動態網頁，也能直接利用 XHR 找到採用 AJAX 技術所發送的連線，而我們要的職缺資訊就藏在這裡 ~\n由於職缺務類別各企業、各平台不盡相同，為幫助同學理解與收斂，將搜尋關鍵詞定為以下 8 大職務類別：\n數據分析師 商業分析師 數據工程師 機器學習工程師\n數據庫工程師 研究人員 軟體開發工程師 維運工程師\n如何篩選出與我們所學相符的職缺？ 運用 Python 中正規表達式(Regular Expression)的模組 re，清洗出職位要求限制包含所學技能的職缺\n如何歸納職務類別？ 需準備一個寫好職務名稱的字典集（相同職務類別不同公司可能會有不同 job title），運用 Python 中字符串模糊匹配庫 FuzzyWuzzy，依據 Levenshtein Distance 算法，計算該 job title 與字典集字詞之間的編輯距離，找出相似度最高的字詞並歸類\n如何計算薪資級距？ 薪資計算方式不同(時薪、日薪、月薪、年薪、按件計酬)且各網站格式不一，需以判斷式先做一輪清洗，又未寫明薪資的職缺數高達 50%，若將面議的薪資數值全數填為 40,000 顯然不合理，因此將資料庫裡寫明薪資的職缺，剔除離群值後，回填最高最低薪之 50 百分位數\n完整程式碼請至 Github ","permalink":"https://uuuna2233.github.io/blog/bigdata-job-part1/","tags":["Python","AWS","MySQL","Tableau"],"title":"大數據相關職缺分析專題 Part 1"},{"categories":["Post"],"contents":"Splunk 基於 MapReduce 分散式架構來處理大量資料，可收集、索引、分析各種來源（如：應用程序、伺服器和設備）生成的實時和歷史數據，且所存取的資料無需正規化（normalize）處理\n於 Linux 安裝 Splunk Reference：Splunk Installation Manual\n測試設備 Log 是否正常監聽 變更 Windows 系統內之事件（如：新增、刪除使用者、修改事件紀錄檔），再到 Splunk 上查看事件是否順利傳送與接收\n建⽴多台電腦之事件監控 本次串聯超過 50 台電腦設備，Splunk 執行個體接收來自轉送器所傳入的其他主機資料，收集其電腦事件紀錄檔 (Log)，藉此監控此集群架構內的資安狀況\n機器資料凡走過必留下痕跡，透過日誌管理、資安稽核、事件監控分析與告警，可預防資安危機，也能對系統狀態（如：效能、異常狀況）監控與故障排除，進行資源規劃以達節流之效\n","permalink":"https://uuuna2233.github.io/blog/splunk-monitoring/","tags":["Splunk"],"title":"Splunk 集中化監控不同主機事件"},{"categories":["Post"],"contents":"Photo by Bridgera\n● 靜態網頁爬蟲：client 向 server 發出 request，server 將網頁文件返回 response，瀏覽器解讀 HTML 並顯示結果，使用單純的 requests 函式庫即可爬取資料\n● 動態網頁爬蟲：client 向 server 發出 request，server 到 Database 存取資料並返回 response，使用 Selenium 套件模擬瀏覽器擷取資料\n透過 API (Application Programming Interface) 實踐 Web API 的資料傳輸格式多為 JSON 與 XML，格式漂亮的 JSON 檔可直接使用，而 XML 則需以 BeautifulSoup、lxml 完成解析\n範例：OMDb API為開放的電影數據庫，基於 RESTful 架構的服務，用於獲取電影訊息\n範例：民報、鉅亨網、自由時報、104薪資情報 等皆可透過開發者工具查看 Network，解析其 API 的參數規律\n使用 GET 的方式 使用 POST 的方式 靜態網頁爬取 與透過 API 爬取方式類似，只是 API 是網站工程師包好特定資料返回，自己以 requests 送出請求是返回整個網頁內容，需再進一步解析 HTML 結構並對其標籤定位\n範例：PTT、TechNews、奇摩股市、中國時報 等皆可透過開發者工具查看 Elements，解析其 HTML 結構\n動態網頁爬取 網頁以滾動捲軸或點擊特殊按鍵來動態載入更多的資料，此時就需以 Selenium 套件，模仿人類行為處理網頁操作\n範例：104人力銀行 職缺搜尋，捲軸滑到最底部會自動載入下一頁，第 16 頁後則需手動點擊載入\n更新：DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead. 訊息指出 find_element_by_* 命令已被棄用，driver.find_element(By.CLASS_NAME, \u0026ldquo;XXXXX\u0026rdquo;)\nReference：\n1. Selenium Locating Elements\n完整程式碼請至 Github ","permalink":"https://uuuna2233.github.io/blog/web-crawling-in-python/","tags":["Python"],"title":"Python 網頁爬蟲 (Web Crawler) 實踐"}]