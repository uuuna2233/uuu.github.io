[{"categories":["Project"],"contents":" 員工的離職原因林林總總，只有兩點最真實：1 錢，沒給到位 2 心，委屈了\n本篇主要講述自動化排程爬蟲程式和服務上雲，並產出視覺化分析讓求職者參考，此專題 Part 1 傳送門\n大數據相關職缺分析專題 Part 1 專題目標 - 讓帶著一顆焦慮的心求職的我們都能順利轉職 透過此次大數據相關的職缺分析，更清晰我們在就業市場的定位，並能根據這半年在養成班的所學，優先精進某幾項核心競爭力，明確自己的求職目標\n大型求職網站的職缺現狀為何？相關資力與薪資級距的關係？有數據人才需求的公司樣態？哪些技能或工具是多數公司的必備條件？\n資料流程 - 蒐集 → 處理 → 儲存 → 調整 → 分析 → 視覺化 處理完資料缺漏、分類、離群值和統一格式後，匯入 pymysql 或 SQLAlchemy 模組並設定連線參數，將資料以 dataframe 形式寫入關聯式資料庫 MySQL\n如何完成 Data Pipeline 自動化排程？ ● Linux 系統由 cron 來控制例行性工作排程，本次專題則以 crontab 指令建立爬蟲程式排程，依據不同網站的更新頻率和職缺數量，排定不同更新週期\n● 資料庫裡的職缺也需定期清理，若該職缺更新日期超過 30 天前，則設定 Event Scheduler 事件排程器執行刪除任務\nP.S. 設定完成後，可下指令即時監測 CRON 是否如期運作 tail -f /var/log/syslog | grep CRON 將服務佈署至雲端 於 AWS 雲端平台架設執行個體 EC2，本次作業系統選擇 Ubuntu\n於雲端虛擬主機中安裝 Anaconda 編輯器執行 Python 命令\n於 AWS 雲端平台使用資料庫服務 RDS，本次資料庫引擎選擇 MySQL\n安裝 GUI 應用程式 MySQL-Workbench 以連線到雲端資料庫\n使用 Lambda 排程自動開啟/停止執行個體，以控制使用量節省成本\n先建立 IAM 政策和執行角色，再編寫 Lambda 函數並佈署，最後選取觸發 Lambda 函數的 EventBridge 規則\n製作可視化分析報表 再將資料匯入 Tableau 前，需進行資料預處理，如：希望呈現不同技能點對應的職缺類型、數量和薪資\nSTEP1 拆分 skill 欄位，將不同技能分為多欄\nSTEP2 將欄索引旋轉為列索引，並置於列索引最內層\nSTEP3 不保留層級為 1 的索引，並加回原 Dataframe\n以 6/24 為基準，收集 30 日內大數據相關職缺，104 和 1111 人力銀行為職缺刊登大宗，且每日更新的職缺數超過 10,000 筆，顯示企業攬才動能高\n整體而言，職位要求技能前 5 名為 sql、python、linux、git、api，此為應扎好基本功的必備技能\n若求職者有意朝〔機器學習工程師〕目標邁進，則可參考以下技能需求持續精進\nP.S. 此次分析因限定在養成班所學技能，所能呈現之技能、工具種類有限\n整體而言，職缺具有一定的薪資水準（月薪 40,000 元起）\n若單獨查看與養成班所學最密切相關的〔數據分析師〕\u0026amp;〔數據工程師〕職缺，其 0-2 年年資也有不錯的薪資行情\n大數據領域發展日新月異，觀念、技術、算法快速迭代，隨時保持一顆探尋答案的好奇心，和市場演進與時俱進 ٩(^ᴗ^)۶\n連結至 網頁儀表板 動態呈現分析結果 完整程式碼請至 Github ","permalink":"https://uuuna2233.github.io/blog/post-2/","tags":["Python","AWS","MySQL","Tableau"],"title":"大數據相關職缺分析專題 Part 2"},{"categories":["Project"],"contents":" 員工的離職原因林林總總，只有兩點最真實：1 錢，沒給到位 2 心，委屈了\n用戶畫像 - 養成班的我們對未來的期待與迷惘 ● 基本資訊：18-29歲 初入職場或轉職者\n● 特徵：學了各種數據分析技能，不停的探索著職涯地圖的我們\n● 痛點：不確定就業市場的用人需求，和自己需要鎖定哪項技能專精\n專題目標 - 讓帶著一顆焦慮的心求職的我們都能順利轉職 透過此次大數據相關的職缺分析，更清晰我們在就業市場的定位，並能根據這半年在養成班的所學，優先精進某幾項核心競爭力，明確自己的求職目標\n大型求職網站的職缺現狀為何？相關資力與薪資級距的關係？有數據人才需求的公司樣態？哪些技能或工具是多數公司的必備條件？\n資料流程 - 蒐集 → 處理 → 儲存 → 調整 → 分析 → 視覺化 至 104、1111、518、YOURATOR、CAKERWSUME 爬取與數據分析相關職缺，將搜尋關鍵詞定為以下 8 大職務類別：\n數據分析師 商業分析師 數據工程師 機器學習工程師\n數據庫工程師 研究人員 軟體開發工程師 維運工程師\n如何篩選出與我們所學相符的職缺？ 運用 Python 中正規表達式(Regular Expression)的模組 re，清洗出職位要求限制包含所學技能的職缺\n如何歸納職務類別？ 需準備一個寫好職務名稱的字典集（相同職務類別不同公司可能會有不同 job title），運用 Python 中字符串模糊匹配庫 FuzzyWuzzy，依據 Levenshtein Distance 算法，計算該 job title 與字典集字詞之間的編輯距離，找出相似度最高的字詞並歸類\n如何計算薪資級距？ 薪資計算方式不同(時薪、日薪、月薪、年薪、按件計酬)且各網站格式不一，需以判斷式先做一輪清洗，又未寫明薪資的職缺數高達 50%，若將面議的薪資數值全數填為 40,000 顯然不合理，因此將資料庫裡寫明薪資的職缺，剔除離群值後，回填最高最低薪之 50 百分位數\n完整程式碼請至 Github ","permalink":"https://uuuna2233.github.io/blog/post-1/","tags":["Python","AWS","MySQL","Tableau"],"title":"大數據相關職缺分析專題 Part 1"},{"categories":["Post"],"contents":"Splunk 基於 MapReduce 分散式架構來處理大量資料，可收集、索引、分析各種來源（如：應用程序、伺服器和設備）生成的實時和歷史數據，且所存取的資料無需正規化（normalize）處理\n於 Linux 安裝 Splunk Reference：Splunk Installation Manual\n測試設備 Log 是否正常監聽 變更 Windows 系統內之事件（如：新增、刪除使用者、修改事件紀錄檔），再到 Splunk 上查看事件是否順利傳送與接收\n建⽴多台電腦之事件監控 本次串聯超過 50 台電腦設備，Splunk 執行個體接收來自轉送器所傳入的其他主機資料，收集其電腦事件紀錄檔 (Log)，藉此監控此集群架構內的資安狀況\n機器資料凡走過必留下痕跡，透過日誌管理、資安稽核、事件監控分析與告警，可預防資安危機，也能對系統狀態（如：效能、異常狀況）監控與故障排除，進行資源規劃以達節流之效\n","permalink":"https://uuuna2233.github.io/blog/post-4/","tags":["Splunk"],"title":"Splunk 集中化監控不同主機事件"},{"categories":["Post"],"contents":"Photo by Bridgera\n● 靜態網頁爬蟲：client 向 server 發出 request，server 將網頁文件返回 response，瀏覽器解讀 HTML 並顯示結果，使用單純的 requests 函式庫即可爬取資料\n● 動態網頁爬蟲：client 向 server 發出 request，server 到 Database 存取資料並返回 response，使用 Selenium 套件模擬瀏覽器擷取資料\n透過 API (Application Programming Interface) 實踐 Web API 的資料傳輸格式多為 JSON 與 XML，格式漂亮的 JSON 檔可直接使用，而 XML 則需以 BeautifulSoup、lxml 完成解析\n範例：OMDb API為開放的電影數據庫，基於 RESTful 架構的服務，用於獲取電影訊息\n範例：民報、鉅亨網、自由時報、104薪資情報 等皆可透過開發者工具查看 Network，解析其 API 的參數規律\n使用 GET 的方式 使用 POST 的方式 靜態網頁爬取 與透過 API 爬取方式類似，只是 API 是網站工程師包好特定資料返回，自己以 requests 送出請求是返回整個網頁內容，需再進一步解析 HTML 結構並對其標籤定位\n範例：PTT、TechNews、奇摩股市、中國時報 等皆可透過開發者工具查看 Elements，解析其 HTML 結構\n動態網頁爬取 網頁以滾動捲軸或點擊特殊按鍵來動態載入更多的資料，此時就需以 Selenium 套件，模仿人類行為處理網頁操作\n範例：104人力銀行 職缺搜尋，捲軸滑到最底部會自動載入下一頁，第 16 頁後則需手動點擊載入\n更新：DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead. 訊息指出 find_element_by_* 命令已被棄用，driver.find_element(By.CLASS_NAME, \u0026ldquo;XXXXX\u0026rdquo;)\nReference：Selenium Locating Elements\n完整程式碼請至 Github ","permalink":"https://uuuna2233.github.io/blog/post-3/","tags":["Python"],"title":"Python 網頁爬蟲 (Web Crawler) 實踐"}]