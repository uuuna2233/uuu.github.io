[{"categories":["Project"],"contents":" 專案目標 透過 Google Sheets API 串接 Google 試算表，以 Python 清洗賽事、伺服器 Log 檔，並將分析結果以 Power BI 視覺化呈現，協助主辦方進行賽事統計與提升伺服器效能\nGoogle Cloud 1. 先到 GCP 建立專案吧！( https://console.cloud.google.com/ )\n2. 再啟用 Google Sheets API\n3. 建立憑證和服務帳戶金鑰\n4. 共用 Google 試算表予服務帳戶，輸入電子郵件並設定編輯者權限\nGoogle sheets API 查看 Google Sheets for Developers，發現每分鐘可調用次數蠻多的，每日讀寫次數則無限制\nBut! 我的帳戶還是被停權了 (ಥ﹏ಥ)，可能是服務帳戶的 API 調用被視為單個帳戶使用，因此即使沒有超出上述使用限制，也無 Response 429: Too many requests，在團隊有多人需要將資料打進同個 Google 試算表的情況下，會需要 Request a higher quota limit! ( https://developers.google.com/sheets/api )\n連結 Google 試算表與設定要使用的工作簿 1. 安裝相關模組\npip install gspread gspread-dataframe oauth2client gspread-formatting 2. 設定操作憑證與範圍 3. 連結試算表並新建當日 sheet 以 Python 清洗賽事、伺服器 log 檔 玩家生存戰況 (Game Log) log 格式 → [19:01:59] [Server thread/INFO]: XXXXXXXX was slain by YYYYYYYY 寫入 Google Sheet 概覽 玩家成就達成 (Advancement Log) log 格式 → [18:43:55] [Server thread/INFO]: XXXXXXXX has made the advancement [Monster Hunter] 伺服器過載紀錄 (Server Log) log 格式 → [18:43:04] [Server thread/WARN]: Can\u0026#39;t keep up! Is the server overloaded? Running 2011ms or 40 ticks behind 寫入 Google Sheet 概覽 防火牆阻擋紀錄 (Ufw Log) log 格式 → Jul 29 15:58:45 mineos-tkldev kernel: [19012.171669] [UFW BLOCK] IN=ztuze7nxnt OUT= MAC=8e:5b:02:67:d9:22:8e:05:bd:df:16:d0:08:00 SRC=172.23.86.214 DST=172.23.249.234 LEN=40 TOS=0x00 PREC=0x00 TTL=64 ID=53307 PROTO=TCP SPT=62711 DPT=8443 WINDOW=4096 RES=0x00 ACK FIN URGP=0 P.S. 系統配發的 id 固定不變，而玩家自定義的 id 可更動\nlog 格式 → [19:21:15] [User Authenticator #25/INFO]: UUID of player XXXXXXXX is c93a7158-d0dd-4d95-85f3-d79ab5155f60 log 格式 → [19:21:15] [Server thread/INFO]: XXXXXXXX[/1.34.43.184:43209] logged in with entity id 17171 at (-151.14292647891168, 78.00430070304934, 159.96209798626114) 寫入 Google Sheet 概覽 計算同時在線玩家人數 (Joined Log) log 格式 → 2022-07-26 [13:26:38] [Server thread/INFO]: XXXXXXXX joined the game 寫入 Google Sheet 概覽\n以 Power BI 視覺化解析賽事結果 Reference：\n1. Google Sheets for Developers\n2. Examples of gspread Usage\n3. Python大數據特訓班(第二版)\n完整程式碼請至 Github ","permalink":"https://uuuna2233.github.io/blog/minecraft-log/","tags":["Python","Power BI"],"title":"Minecraft 麥塊競賽 Log 解析"},{"categories":["Project"],"contents":" 業務場景 AWS 擁有複雜度超高的計費方式，帳單報告多達 150 個欄位，不同的雲服務、產品規格、用量狀況、計價模式、生效時間等等讓人眼花撩亂 (◎.◎)\n此次專案目標為協助雲端平台服務商從無到有建立會員帳務系統，讓 客戶可以清楚地找到關注的訊息 (ex 每日成本用量，折扣使用狀況)，也讓企業能動態更新帳務，將此業務計算自動化，減少在此投入的人工，進而 專注於雲端整合服務的本業上\n雲端架構 ● CUR：內含最完整的 AWS 成本和用量數據，依產品代碼、項目類型、使用時間逐項列出帳戶或組織層級的用量\n● S3：CUR 會向 S3 bucket 提交以小時細分的帳務報告，每天更新一次，於次月第 7 日提供最終報告\n● Lambda：以 trigger 接收 S3 Event，當有新帳單存入 S3，即啟動相對應的 Funtion，對帳單做一系列的處理並存入 DB\n● CloudWatch：監控 Lambda Funtion 運行狀況，並為 LOG 設定警示條件，在異常時透過 Alarm 通知 Admin\n● MySQL - 開發階段在 EC2 自建 DB 以節省成本，正式上線可考量採用 AWS RDS 服務建立 MySQL\n● Elastic IP：為針對動態雲端運算設計的靜態 IPv4 地址，關聯至指定的 EC2，公有 IP 即不會異動\n● VPC：為快速進行開發，目前配有一個 Public Subnet，正式上線需將服務拆分不同 Instance 並置於 Private Subnet\n如何查看最新報告? S3 可以找到 Manifest.json 檔案，清單紀錄了報告的架構，reportKeys 為最新報告文件的 S3 文件路徑\n如何確定最終報告? 如果是最終版，bill/InvoiceId 會填入 \u0026lsquo;AWS invoice ID\u0026rsquo;，bill/BillType 則填入 \u0026lsquo;Anniversary\u0026rsquo; 清洗入庫效能比較 因業務需求，帳單需拆分為 Table billing_1、Table billing_2\n先判斷 A 欄位的類型，再判斷 B 欄位和 C 欄位一致與否\n● Option 1：使用迴圈讀取並逐條判斷，處理效能低落，每秒約處理 2 列，需耗時 21 小時\n● Option 2：使用 dataframe 一次處理，處理效能明顯優於前者，15 萬條數據 10 秒內完成入庫\n資料庫架構 設計完整的資料庫系統，要考量的細節很多，圍繞在以下列舉的考量，皆非常考驗著業務理解和熟悉資料庫背後工作原理\u0026hellip;\n應用業務的需求上： ex 客戶關心什麼內容 / 前端頁面呈現邏輯 / 模擬內部作業流程（如：團隊在 Key in 客戶訊息、折扣等內容時夠不夠直覺好用） 資料庫效能與容量： ex 前端如何透過 API 向後端請求資料 / 資料存取類型與編碼 / 索引、外鍵如何設計 / 應累加或定期更新哪些資訊 後續維護和需求延展：ex 使用者的權限管理 / 應依照客戶或是週期分庫分表 / 未來業務變化需更動表格配置時夠不夠彈性 資料庫正規化 (Normalization) 第一正規化 (1NF)：每個欄位只能有一個基元值 (Atomic)，表中有主鍵，而其它欄位都相依於主鍵\\ 第二正規化 (2NF)：每個非鍵屬性必須「完全相依」於主鍵，亦即將「部分功能相依」的欄位分割，再另外組成新 Table\\ 第三正規化 (3NF)：各欄位與主鍵之間沒有「遞移相依」的關係，亦即將「遞移相依」的欄位分割，再另外組成新 Table AWS 原始帳單已符合每個欄位僅有單一值，將 identity/LineItemId 和 identity/TimeInterval 當作聯合主鍵，product/sku 為產品的獨特代碼，可以此欄為作為另一張表的主鍵，利用「外鍵」來連接主表\n事件排程器 (Event Scheduler) delimiter $$ CREATE [DEFINER = user] EVENT [IF NOT EXISTS] event_name ON SCHEDULE schedule [ON COMPLETION [NOT] PRESERVE] [ENABLE | DISABLE | DISABLE ON SLAVE] [COMMENT \u0026#39;string\u0026#39;] DO event_body; end $$ delimiter; schedule: { AT timestamp [+ INTERVAL interval] ... | EVERY interval [STARTS timestamp [+ INTERVAL interval] ...] [ENDS timestamp [+ INTERVAL interval] ...] } interval: quantity {YEAR | QUARTER | MONTH | DAY | HOUR | MINUTE | WEEK | SECOND | YEAR_MONTH | DAY_HOUR | DAY_MINUTE | DAY_SECOND | HOUR_MINUTE | HOUR_SECOND | MINUTE_SECOND} SET GLOBAL event_scheduler = ON; identity/LineItemId 項目可能會依照不同的報告時間改變，因此需每日以新報告覆蓋原表格\n而以此表格延伸出的數張報表亦需每日更新，即需啟用 SQL 排程來完成 設計索引、外鍵 (Index、Foreign Key) 適當的索引可以加快數據訪問，提高查詢效能；而外鍵的約束用於防止非法數據插入，更新、刪除時也能保持表格一致性\n前端經常返回的變數為 \u0026lsquo;business_tax\u0026rsquo;，即經常使用 \u0026lsquo;business_tax\u0026rsquo; 執行條件判斷，即能為它建立索引\n若發現 query 速度很慢，在 query 敘述前使用 EXPLAIN，可以解析查詢狀況，進而調優 SQL 語法\n多階層子查詢、結合查詢 (Subquery、Join) 查詢如何撰寫會大幅度的影響效能，尤其在複雜的巢狀子查詢、多次結合查詢\n比如需求 JOIN 4 張表，是要 A JOIN B 後，再 JOIN C，最後 JOIN D，或者 A JOIN B、C JOIN D，再 JOIN 前述兩表\u0026hellip;\n該 WHERE A.ID 或者 WHERE B.ID，是否使用了暫時表格 (Temporary Table)\u0026hellip;\n資料庫是大數據從業人員必備技能，雖入門容易，但學習曲線到某一程度會大幅趨緩，需投入更多時間才能成長，關注的點也不僅在於「查詢的出正確結果」就好，未來得持續精進如災難復原、日誌監控、Cluster配置、SQL injection\u0026hellip;等知識\nReference：\n1. AWS Data dictionary\n2. MySQL Normalization\n3. MySQL Event Scheduler\n","permalink":"https://uuuna2233.github.io/blog/aws-billing-system/","tags":["Python","AWS","MySQL","Linux"],"title":"於 AWS 雲端平台搭建會員帳務系統"},{"categories":["Post"],"contents":"● 非關聯式資料庫 (NoSQL)\nMongoDB 是文件導向式資料庫，儲存由 key-value 配對而成的資料，無需事先定義資料型態，對於不同表徵的資料，都能快速存進資料庫\n● BSON 格式\n以 JSON 字串存進資料庫，MongoDB 內部會將 JSON 轉成 BSON，由 text 格式轉成 binary 格式，加快文件解析速度，亦能儲存非文字資料\n● _id 欄位\nMongoDB 會為每筆資料自動建立 _id 欄位，預設內容為 ObjectID，可視為主索引鍵，用於區分不同 Document，ObjectID 前 4 個 bytes 為 Document 產生時的時間戳記\nRDBMS MongoDB Remarks Database Database 資料庫 Table Collection 資料表/聚集 Row Document 資料/文件 Column Field 欄位/鍵名 Primary _id 主鍵/主索引 View View 視觀表 安裝 MongoDB on Windows MongoDB 可於本地安裝 Community Server，也能使用 Atlas 在 AWS、Azure 或 GCP 建立雲端託管服務\n本次選擇本地 (Windows) 下載 msi 安裝檔，預設執行檔會放在 C:\\Program Files\\MongoDB\\Server\\[version]\\bin，此次將資料庫存放到 D 槽\n將 Windows 服務裡的 MongoDB 項目改為手動（正式上線使用前，改成手動以免重開機後 MongoDB server 自動啟動)\n可以安裝 GUI 工具 compass 和 Shell 工具 mongosh，直接操作 MongoDB 打開命令提示字元並輸入 mongosh.exe 以連接 MongoDB，指令相當於 mongosh \u0026ldquo;mongodb://localhost:27017\u0026rdquo;\n安裝 PyMongo 函式庫 皆為 MongoDB 官方提供，目前只支援 Python3\npymongo[srv] 讓 Python 連線至 Atlas\n$ pip install pymongo $ pip install \u0026#34;pymongo[srv]\u0026#34; 存取空氣品質指標(AQI)資料 新增資料 政府資料開放平臺 免費提供各式各樣的資料集 (Open Data)，涵蓋生育保健、開創事業、購屋遷徙等多種議題，對數據分析人員而言這真是挖寶的好地方，此次以 空氣品質指標(AQI) 為資料來源，分析近期台灣空汙狀況 ~\nP.S. 此處未將各空汙指標轉為數字型態再存進資料庫，後續將以 MongoDB 語法處理\nReference：政府資料開放平臺\n查詢資料 空氣品質指標(AQI)資料中有兩個欄位名稱 (pm2.5 和 2.5_avg) 含 ($) 和 (.)，雖然 MongoDB v6.0 官方說明文檔顯示在 v5.0 後的版本支持字段名稱含 ($) 和 (.)，但實測仍然有問題\n無法解析帶有 (.) 的 field，因此需在匯入 MongoDB 前，先將 pm2.5 和 2.5_avg 更名為 pm2_5 和 pm2_5_avg\nMongoDB 透過正規表示法設定多樣化的查詢條件：\nregex 運算子 {\u0026lt;field\u0026gt;:{\u0026#39;$regex\u0026#39;: \u0026#39;pattern\u0026#39;, \u0026#39;$options\u0026#39;: \u0026lt;options\u0026gt;}} 正則表達式對象 {\u0026lt;field\u0026gt;: /pattern/\u0026lt;options\u0026gt;} 欲查詢彰、雲、嘉地區觀測站 pm2.5 指數，利用包含運算子 $in，查詢符合陣列中元素的資料，因地區可能包含縣、市，這裡想結合模糊查詢實現查詢\n$in 只能使用正則表達式對象，以下指令可於 mongosh 執行，但在 python 環境不適用（因陣列中的字串未加引號會報錯）：\n{\u0026#39;county\u0026#39;:{\u0026#39;$in\u0026#39;:[/彰化/, /雲林/, /嘉義/]}} pymongo 提供兩種方法：\n$regex 對單個關鍵詞進行模糊查詢 {\u0026#39;$regex\u0026#39;:\u0026#39;彰化\u0026#39;} re.compile() 對多個關鍵詞進行模糊查詢 {\u0026#39;$in\u0026#39;:[re.compile(\u0026#39;彰化\u0026#39;), re.compile(\u0026#39;雲林\u0026#39;), re.compile(\u0026#39;嘉義\u0026#39;)]} 運用 where 語句 欲查詢 AQI 值大於 50 小於 100 的資料，但 AQI 欄位型態為 String 無法做比較運算，因此可透過 pymongo 提供的特殊函數 where() 函數配合 JavaScript 的型別轉換函數實現\n修改資料 若想更新某一個監測站資料，但不確定資料庫是否存在此監測站，可透過 upsert Parameter：\nWhen true, update() either:\n● Creates a new document if no documents match the query\n● Updates a single document that matches the query.\nDefaults to false, which does not insert a new document when no match is found.\n刪除資料 將方才練習更新的資料刪除，可直接指定 \u0026lsquo;sitename\u0026rsquo;，也能藉由「刪除最新一筆」資料來實現\nReference：\n1. Install MongoDB Community Edition on Windows\n2. Field Names with Periods (.) and Dollar Signs ($)\n3. 朱克剛(2022)。MongoDB 5.x實戰應用\n完整程式碼請至 Github ","permalink":"https://uuuna2233.github.io/blog/mongodb-in-python-part1/","tags":["Python","MongoDB"],"title":"MongoDB 存取全台空氣品質指標 Part 1"},{"categories":["Project"],"contents":" 員工的離職原因林林總總，只有兩點最真實：1 錢，沒給到位 2 心，委屈了\n本篇主要講述自動化排程爬蟲程式和服務上雲，並產出視覺化分析讓求職者參考，此專題 Part 1 傳送門\n大數據相關職缺分析專題 Part 1 專題目標 - 讓帶著一顆焦慮的心求職的我們都能順利轉職 透過此次大數據相關的職缺分析，更清晰我們在就業市場的定位，並能根據這半年在養成班的所學，優先精進某幾項核心競爭力，明確自己的求職目標\n大型求職網站的職缺現狀為何？相關資力與薪資級距的關係？有數據人才需求的公司樣態？哪些技能或工具是多數公司的必備條件？\n資料流程 - 蒐集 → 處理 → 儲存 → 調整 → 分析 → 視覺化 處理完資料缺漏、分類、離群值和統一格式後，匯入 pymysql 或 SQLAlchemy 模組並設定連線參數，將資料以 dataframe 形式寫入關聯式資料庫 MySQL\n如何完成 Data Pipeline 自動化排程？ ● Linux 系統由 cron 來控制例行性工作排程，本次專題則以 crontab 指令建立爬蟲程式排程，依據不同網站的更新頻率和職缺數量，排定不同更新週期\n● 資料庫裡的職缺也需定期清理，若該職缺更新日期超過 30 天前，則設定 Event Scheduler 事件排程器執行刪除任務\nP.S. 設定完成後，可下指令即時監測 CRON 是否如期運作 tail -f /var/log/syslog | grep CRON 將服務佈署至雲端 於 AWS 雲端平台架設執行個體 EC2，本次作業系統選擇 Ubuntu\n於雲端虛擬主機中安裝 Anaconda 編輯器執行 Python 命令\n於 AWS 雲端平台使用資料庫服務 RDS，本次資料庫引擎選擇 MySQL\n安裝 GUI 應用程式 MySQL-Workbench 以連線到雲端資料庫\n使用 Lambda 排程自動開啟/停止執行個體，以控制使用量節省成本\n先建立 IAM 政策和執行角色，再編寫 Lambda 函數並佈署，最後選取觸發 Lambda 函數的 EventBridge 規則\n製作可視化分析報表 再將資料匯入 Tableau 前，需進行資料預處理，如：希望呈現不同技能點對應的職缺類型、數量和薪資\nSTEP1 拆分 skill 欄位，將不同技能分為多欄\nSTEP2 將欄索引旋轉為列索引，並置於列索引最內層\nSTEP3 不保留層級為 1 的索引，並加回原 Dataframe\n以 6/24 為基準，收集 30 日內大數據相關職缺，104 和 1111 人力銀行為職缺刊登大宗，且每日更新的職缺數超過 10,000 筆，顯示企業攬才動能高\n整體而言，職位要求技能前 5 名為 sql、python、linux、git、api，此為應扎好基本功的必備技能\n若求職者有意朝〔機器學習工程師〕目標邁進，則可參考以下技能需求持續精進\nP.S. 此次分析因限定在養成班所學技能，所能呈現之技能、工具種類有限\n整體而言，職缺具有一定的薪資水準（月薪 40,000 元起）\n若單獨查看與養成班所學最密切相關的〔數據分析師〕\u0026amp;〔數據工程師〕職缺，其 0-2 年年資也有不錯的薪資行情\n大數據領域發展日新月異，觀念、技術、算法快速迭代，隨時保持一顆探尋答案的好奇心，和市場演進與時俱進 ٩(^ᴗ^)۶\n連結至 網頁儀表板 動態呈現分析結果 完整程式碼請至 Github ","permalink":"https://uuuna2233.github.io/blog/bigdata-job-part2/","tags":["Python","AWS","MySQL","Tableau","Linux"],"title":"大數據相關職缺分析專題 Part 2"},{"categories":["Project"],"contents":" 員工的離職原因林林總總，只有兩點最真實：1 錢，沒給到位 2 心，委屈了\n用戶畫像 - 養成班的我們對未來的期待與迷惘 ● 基本資訊：18-29歲 初入職場或轉職者\n● 特徵：學了各種數據分析技能，不停的探索著職涯地圖的我們\n● 痛點：不確定就業市場的用人需求，和自己需要鎖定哪項技能專精\n專題目標 - 讓帶著一顆焦慮的心求職的我們都能順利轉職 透過此次大數據相關的職缺分析，更清晰我們在就業市場的定位，並能根據這半年在養成班的所學，優先精進某幾項核心競爭力，明確自己的求職目標\n大型求職網站的職缺現狀為何？相關資力與薪資級距的關係？有數據人才需求的公司樣態？哪些技能或工具是多數公司的必備條件？\n資料流程 - 蒐集 → 處理 → 儲存 → 調整 → 分析 → 視覺化 至 104、1111、518、YOURATOR、CAKERWSUME 爬取與數據分析相關職缺，將搜尋關鍵詞定為以下 8 大職務類別：\n數據分析師 商業分析師 數據工程師 機器學習工程師\n數據庫工程師 研究人員 軟體開發工程師 維運工程師\n如何篩選出與我們所學相符的職缺？ 運用 Python 中正規表達式(Regular Expression)的模組 re，清洗出職位要求限制包含所學技能的職缺\n如何歸納職務類別？ 需準備一個寫好職務名稱的字典集（相同職務類別不同公司可能會有不同 job title），運用 Python 中字符串模糊匹配庫 FuzzyWuzzy，依據 Levenshtein Distance 算法，計算該 job title 與字典集字詞之間的編輯距離，找出相似度最高的字詞並歸類\n如何計算薪資級距？ 薪資計算方式不同(時薪、日薪、月薪、年薪、按件計酬)且各網站格式不一，需以判斷式先做一輪清洗，又未寫明薪資的職缺數高達 50%，若將面議的薪資數值全數填為 40,000 顯然不合理，因此將資料庫裡寫明薪資的職缺，剔除離群值後，回填最高最低薪之 50 百分位數\n完整程式碼請至 Github ","permalink":"https://uuuna2233.github.io/blog/bigdata-job-part1/","tags":["Python","AWS","MySQL","Tableau"],"title":"大數據相關職缺分析專題 Part 1"},{"categories":["Post"],"contents":"Splunk 基於 MapReduce 分散式架構來處理大量資料，可收集、索引、分析各種來源（如：應用程序、伺服器和設備）生成的實時和歷史數據，且所存取的資料無需正規化（normalize）處理\n於 Linux 安裝 Splunk Reference：Splunk Installation Manual\n測試設備 Log 是否正常監聽 變更 Windows 系統內之事件（如：新增、刪除使用者、修改事件紀錄檔），再到 Splunk 上查看事件是否順利傳送與接收\n建⽴多台電腦之事件監控 本次串聯超過 50 台電腦設備，Splunk 執行個體接收來自轉送器所傳入的其他主機資料，收集其電腦事件紀錄檔 (Log)，藉此監控此集群架構內的資安狀況\n機器資料凡走過必留下痕跡，透過日誌管理、資安稽核、事件監控分析與告警，可預防資安危機，也能對系統狀態（如：效能、異常狀況）監控與故障排除，進行資源規劃以達節流之效\n","permalink":"https://uuuna2233.github.io/blog/splunk-monitoring/","tags":["Splunk"],"title":"Splunk 集中化監控不同主機事件"},{"categories":["Post"],"contents":"Photo by Bridgera\n● 靜態網頁爬蟲：client 向 server 發出 request，server 將網頁文件返回 response，瀏覽器解讀 HTML 並顯示結果，使用單純的 requests 函式庫即可爬取資料\n● 動態網頁爬蟲：client 向 server 發出 request，server 到 Database 存取資料並返回 response，使用 Selenium 套件模擬瀏覽器擷取資料\n透過 API (Application Programming Interface) 實踐 Web API 的資料傳輸格式多為 JSON 與 XML，格式漂亮的 JSON 檔可直接使用，而 XML 則需以 BeautifulSoup、lxml 完成解析\n範例：OMDb API為開放的電影數據庫，基於 RESTful 架構的服務，用於獲取電影訊息\n範例：民報、鉅亨網、自由時報、104薪資情報 等皆可透過開發者工具查看 Network，解析其 API 的參數規律\n使用 GET 的方式 使用 POST 的方式 靜態網頁爬取 與透過 API 爬取方式類似，只是 API 是網站工程師包好特定資料返回，自己以 requests 送出請求是返回整個網頁內容，需再進一步解析 HTML 結構並對其標籤定位\n範例：PTT、TechNews、奇摩股市、中國時報 等皆可透過開發者工具查看 Elements，解析其 HTML 結構\n動態網頁爬取 網頁以滾動捲軸或點擊特殊按鍵來動態載入更多的資料，此時就需以 Selenium 套件，模仿人類行為處理網頁操作\n範例：104人力銀行 職缺搜尋，捲軸滑到最底部會自動載入下一頁，第 16 頁後則需手動點擊載入\n更新：DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead. 訊息指出 find_element_by_* 命令已被棄用，driver.find_element(By.CLASS_NAME, \u0026ldquo;XXXXX\u0026rdquo;)\nReference：\n1. Selenium Locating Elements\n完整程式碼請至 Github ","permalink":"https://uuuna2233.github.io/blog/web-crawling-in-python/","tags":["Python"],"title":"Python 網頁爬蟲 (Web Crawler) 實踐"}]